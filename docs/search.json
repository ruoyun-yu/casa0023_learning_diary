[
  {
    "objectID": "week9.html",
    "href": "week9.html",
    "title": "week7",
    "section": "",
    "text": "9.1 Summary\nThis week’s lecture focused on SAR\n1. What is SAR \nSynthetic Aperture Radar (SAR) is an active sensor. Compared to optical imagery, which relies on spectral characteristics, SAR imagery is based on backscatter. Short-wavelength sensors provide higher spatial resolution images but have weaker penetration capabilities. Long-wavelength sensors produce lower resolution images but can penetrate vegetation or thin surface layers.\n2. Radar Incidence Direction & Polarization \nRadar incidence direction and polarization are two different concepts. SAR acquires ground information from a side-looking angle, which helps avoid strong backscatter interference and provides terrain and target structure details. Sentinel-1 operates on both ascending and descending orbits. Imaging from different angles leads to different radar backscatter characteristics for buildings, so images from different orbits need to be analyzed separately. Polarization refers to the direction in which the electromagnetic wave vibrates.\n3. Scattering Mechanism \nThe brightness of a SAR image depends on the scattering mechanism of the surface; different surface types scatter radar waves in different ways. By using various polarizations, different types of ground information can be obtained. \n\n\n\n\n\n\n\n\n\nScattering Type\nSurface Occurrence\nImage Brightness\nPolarization Characteristic\n\n\n\n\nSurface Scattering\nWater, smooth ground\nVery dark (black)\nLow backscatter\n\n\nVolume Scattering\nForests, vegetation\nModerate (brighter)\nVH polarization is obvious\n\n\nDouble-Bounce Scattering\nUrban buildings, vertical structures\nVery bright\nVV polarization is obvious\n\n\n\n4. Information Contained in SAR \nSAR data contains two main types of information: amplitude and phase. Amplitude indicates the strength of the returned radar signal, while phase records the wave peaks and troughs of the returned electromagnetic signal.\n5. InSAR \nInSAR is a method that uses phase information to measure small ground changes. It can detect ground displacements on the centimeter scale. For example, if the ground level changes (such as subsidence due to an earthquake), the phase of the backscattered signal will change slightly. By comparing the phase difference between two images, the direction and magnitude of ground movement can be measured. InSAR can also be used to generate digital elevation models, providing high-precision maps of the ground or buildings. \n   Methods for detecting abnormal changes: \n   - Mean Analysis: Calculate the average backscatter intensity before and after the change; significant differences can be considered abnormal changes. \n   - Variance Analysis: Calculate the standard deviation before and after the change; if the new mean exceeds the range of the standard deviation, it is considered an abnormal change.\n\n\n9.2 Application\nSAR can record both phase and intensity information to study the deformation of objects, and is commonly used to monitor object movement. Urban and natural environments have different characteristics, so other methods and data are needed to supplement SAR’s limitations in research.\nIn complex urban environments with dense buildings, SAR images can suffer from geometric distortions, making object segmentation difficult. Chisheng Wang and others introduced an object-oriented MT-InSAR (Obj-InSAR) framework, combining SAR images and 3D point clouds to generate 4D deformation point cloud data. The “4D” refers to 3D in space and 1D in deformation. The research steps include Object Segmentation, Parameter Estimation, Fine Alignment & Geopositioning, and Generation of InSAR 4D Point Cloud. First, orthophotos are generated from 3D point clouds in the geographic coordinate system, then target segmentation is performed using the Segment Anything Model (SAM). By using a 3D lookup table to relate 3D coordinates and SAR pixels, each 3D geographic coordinate (X, Y, Z) can correspond to a SAR image pixel (r, a). This method overcomes the issue of losing facade information in complex urban environments caused by the original 2D lookup table, achieving precise target matching. Permanent Scatterers (PS) are selected to construct a hybrid network (Delaunay + fully connected network) for parameter estimation. Time series analysis is performed to remove errors and extract nonlinear deformation information, ultimately generating an accurate 4D deformation point cloud.\nIn natural environments, such as measuring glacier movement, SAR images are prone to decorrelation due to atmospheric interference. In such cases, DInSAR, which relies on phase information, is not suitable. Instead, Offset Tracking, based on image intensity information, can be used. Offset Tracking selects two matching windows from two images, computes the similarity between them using Normalized Cross-Correlation, and finds the best matching point to obtain the displacement field of the object. Fan, Jinghui, and others compared the accuracy differences between the offset tracking technique based on SAR and point cloud calculation based on Terrestrial Laser Scanners. The study shows that the offset tracking technique depends on image quality and is more reliable when extracting large-scale glacier velocity fields.\n\n\n9.3 Reflection\nIn my previous studies, I had some understanding of SAR, but it was limited to its operational principles. This week’s learning introduced me to the ability of SAR sensors to extract phase information and the value of InSAR in detecting displacement. This made me think about how this technology could be applied to urban village redevelopment in China. Urban village redevelopment is a major urban renewal strategy, involving the demolition and reconstruction or renovation of villages within built-up urban areas. A key challenge in planning redevelopment is accurately measuring the floor area of existing buildings. Currently, this is often done through manual surveys, which are costly in terms of both labor and resources. Through this week’s learning, I began to consider whether SAR data could be used for preliminary area estimation.\nFurthermore, I believe SAR could also be useful for monitoring illegal building extensions. Since compensation amounts are often tied to building area, some individuals engage in unauthorized construction. Existing monitoring methods mainly rely on detecting changes in surface materials, such as identifying whether bare land has been covered with concrete. To evade detection, many illegal extensions maintain the original rooftop material, making them difficult to identify using conventional methods. However, if InSAR techniques were used to analyze time-series images and detect height changes in buildings, it might offer a more effective way to monitor such illegal construction activities. Urban management departments could potentially develop models based on this approach to enhance urban governance.\n\n\n9.4 References\nWang, C. et al. (2024) A New Object-Oriented SAR Interferometry Framework for Monitoring Urban Deformation. IEEE transactions on geoscience and remote sensing. [Online] 621–11.\nFan, J. et al. (2019) Monitoring and Analyzing Mountain Glacier Surface Movement Using SAR Data and a Terrestrial Laser Scanner: A Case Study of the Himalayas North Slope Glacier Area. Remote sensing (Basel, Switzerland). [Online] 11 (6), 625-."
  },
  {
    "objectID": "week7.html",
    "href": "week7.html",
    "title": "week7",
    "section": "",
    "text": "7.1 Summary\nThis week’s lecture focused on classification\n1. Classification process\n\nConfirm training data and forecast data\nEstablish a classification model\nGenerate classification results and evaluate accuracy\n\n\n2. Classification type\n\nSupervised classification: Manually provide training samples, such as decision trees and support vector machines (SVM).\nUnsupervised classification: there is no manual annotation, only the number of categories is set, and the algorithm automatically clustering data, such as ISODATA method\nTraditional classifiers & Machine learning classifiers:\n\n\nTraditional classifiers (not model-based) : Maximum Likelihood, Parallelpiped, ISODATA, etc., mainly divide data based on feature space.\nMachine learning classifiers (model-based) : decision trees, SVMS, etc., use training data to build more complex classification models.\n\n3. Classification Tree& Regression Tree\n\nClassification Tree It is mainly used for the prediction of discrete or classified data with the Impurity impurity impurity selected as the root node.\nRegression Tree It is mainly used for prediction of continuous data and is used when linear regression does not fit the data well. The partition point with the smallest residual sum of squares (SSR) was selected as the split node\nMethods to prevent overfitting\n\n\nSets the maximum tree depth\nSets the minimum number of leaf nodes\nWeakest Link Pruning\nCross-Validation e.g. k-fold cross-validation\nRandom Forest\nSpatial overfitting: If the training set is geographically close to the test set, it may lead to overfitting and affect the model generalization ability.\n\nSets a minimum distance threshold to ensure that there is space between the training and validation data.\nCluster the training data to ensure that there is sufficient diversity among different datasets\nAccuracy does not equal predictive power: Models without spatial autocorrelation are usually more accurate than models with spatial autocorrelation, but this does not mean that they are more predictive.\n\n\n\nEvaluation Decision trees are highly interpretable, while deep learning is highly accurate but highly black box.\n\n4. Support Vector Machine (SVM)\n\nEssentially similar to regression, finding the optimal Hyperplane divides the data. The C parameter controls the decision boundary width of the hyperplane (allowing certain misclassification).\nThe Gamma parameter controls the range of influence, with high values considered local and low values considered global. Kernel Trick allows nonlinear data to be mapped so that it is separable in high dimensional space.\nAutomatically tune C and Gamma parameters via Grid Search.\nSVM ensures optimal separation by maximizing the distance from the support vector of the two classes to the hyperplane.\n\n\n\n7.2 Application\nClassification can refer to crop type classification or urban land use classification. In practical applications, supervised classification can be implemented using various algorithms. A review of the literature reveals that different data and implementation processes are adopted depending on the objective. Hyperspectral imagery can be utilized, and preprocessing before image classification can improve classification accuracy. If low cost and speed are prioritized, noise-resistant algorithms can be used, with OSM as the training data, eliminating the need for manual selection of training data.\n\n\n\n\n\n\n\n\nArticle Title\nLand Use Classification of Hyperspectral Data by Spectral Angle Mapper and Support Vector Machine in Humid Tropical Region of India\nIntegrating OpenStreetMap Crowdsourced Data and Landsat Time-Series Imagery for Rapid Land Use/Land Cover (LULC) Mapping: Case Study of the Laguna de Bay Area of the Philippines\n\n\n\n\nStudy Area Characteristics\nCloudy and foggy, diverse crop types\nCloudy and foggy, rapid land use changes\n\n\nResearch Content\nUsing SAM and SVM with hyperspectral imagery to classify nine types of crop land use in the study area and comparing the classification accuracy of both methods.\nSelecting NB, C4.5, and RF (+SMOTE) as noise-resistant classification algorithms with Landsat multispectral imagery. Automatically extracting training samples from OSM and conducting land use classification with preprocessed data.\n\n\nRemote Sensing Data\nHyperspectral data from EO-1 Hyperion\nMulti-temporal Landsat 8 + OSM crowdsourced data\n\n\nData Preprocessing\n\nSensor calibration: Removing overlapping bands.\nExtracting radiance values from the imagery.\nAtmospheric correction: Using the FLAASH module to eliminate low reflectance bands.\nGeometric correction: Using IRS-P6 LISS III imagery as a reference.\nDimensionality reduction: Applying Principal Component Analysis (PCA) to optimize computational complexity.\n\nLandsat data is directly used without preprocessing.\n\n\nImage Classification\n\nGeometric-distance-based classification: Spectral Angle Mapper (SAM).\nSupervised classification: Support Vector Machine (SVM).\n\n\nConverting categorical data: OSM-LU and OSM-N categories are converted into four, five, and six land cover classes.\nExtracting training pixels: OSM polygons are randomly split 50/50 to create independent training and validation datasets. Random points are generated within each training polygon to serve as training pixels for classification.\nSupervised image classification:\nNB, C4.5, RF (+SMOTE).\n\n\n\nAccuracy Evaluation\nBased on ground truth data, calculating overall accuracy (OA) and Kappa coefficient.\nUsing high-resolution Google Earth imagery to determine the actual land use/land cover class for each point. Calculating overall accuracy (OA), producer’s accuracy (PA), and user’s accuracy (UA).\n\n\nResearch Findings\nSVM outperforms SAM.\nRF (+SMOTE) performs best in the four-class classification.\n\nRF is more effective in classifying “impervious surfaces” and “farmland.”\nNB performs better in classifying “orchards” and “forests.”\nThe SMOTE technique significantly improves classification accuracy, especially in decision-tree-based algorithms.\n\n\n\n\n\nSAM vs SVM\nSAM classifies by calculating the Angle between the spectral vector of each pixel in the image and the spectral vector of a spectral library (or training sample) of a known class. The smaller the spectral Angle, the more similar the pixel is to the class. SVM separates different categories of data by finding an optimal hyperplane in the feature space. The classification effect of SVM is better than that of SAM mainly in the following aspects: Adaptability: SVM can handle complex, non-linear data, map the data to high-dimensional Spaces through kernel functions, and handle more complex boundaries. SAM is suitable for data with obvious spectral characteristics and linear separability, and the effect is simple and the noise is small. Noise robustness: SVM deals with noise through regularization and kernel functions, which can reduce errors. SAM is sensitive to noise, especially when the data quality is poor. Classification accuracy: SVM can find more complex decision boundaries and is generally more accurate in remote sensing classification. SAM is only suitable for data with large spectral differences.\nHow does the synthetic Minority oversampling technique work when dealing with training data imbalances?\nSMOTE balances the training data set by inserting synthetic samples between a few class samples. The specific steps are as follows:\n\n\n\nCalculate the distance between samples: For each minority sample, calculate the Euclidean distance between it and other minority samples.\nSelection of interpolation points: The interpolation points are selected according to the distance, usually the nearest neighbor sample whose distance is less than a certain threshold.\nGeneration of new samples: at the selected interpolation points, a new minority class sample is generated by linear interpolation or other methods.\n\n\n\nSources of data noise\nLandsat data is affected by clouds and has attribute noise The error of OSM data when combining categories and the inaccurate boundary of original OSM data when delimited in the field lead to class noise\n\n\n\n7.3 Reflection\nThis week’s learning content helped me realize that classified data is not only used for classifying ground objects but also for classifying gases. This broadened my understanding of the potential applications of remote sensing imagery. Traditionally, urban air quality monitoring has relied heavily on data collected from monitoring stations, which comes with high economic costs. In some regions, governments have even closed monitoring stations near pollution sources to conceal air pollution levels. This practice affects the objectivity and accuracy of related research. If remote sensing imagery were used to monitor urban emissions, it could improve data accuracy while ensuring the public’s right to information.\nAdditionally, I learned about the principles of supervised and unsupervised classification this week. Supervised classification involves providing training samples as a foundation for the model to learn and classify, while unsupervised classification automatically groups data based on spectral characteristics. With the advancement of large language models, can AI replace experts in selecting training samples? If this approach becomes feasible, would it reduce the cost of data analysis and empower data usage in less developed regions, or would it exacerbate inequalities in data access? This seems like a meaningful research topic to explore.\n\n\n7.4 References\nGopinath, G. et al. (2020) Landuse classification of hyperspectral data by spectral angle mapper and support vector machine in humid tropical region of India. Earth science informatics. [Online] 13 (3), 633–640.\nJohnson, B. A. & Iizuka, K. (2016) Integrating OpenStreetMap crowdsourced data and Landsat time-series imagery for rapid land use/land cover (LULC) mapping: Case study of the Laguna de Bay area of the Philippines. Applied geography (Sevenoaks). [Online] 67140–149."
  },
  {
    "objectID": "week4.html",
    "href": "week4.html",
    "title": "week4",
    "section": "",
    "text": "4.1 Summary\nLondon is a green city, with over 47% of its area classified as green or blue and more than 8 million trees. The total asset value of London’s public green spaces exceeds £91 billion, with an estimated annual value of £5 billion. Additionally, access to green spaces helps Londoners save approximately £950 million annually in health costs. Besides, parks and green spaces in England contribute £6.6 billion annually in benefits related to health, climate change, and the environment.\n\n\nHowever, due to population growth and land development, the extent of green spaces in both London and England has significantly declined over the past decade. A 2019 Committee on Climate Change report stated that the proportion of green space in England’s urban areas decreased from 63% in 2001 to 55% in 2018.\nUrban resilience refers to a city’s ability to withstand and recover from shocks and stresses, encompassing economic, social, environmental, and institutional resilience. 《World Cities Report 2022: Envisaging the Future of Cities》highlights that investing in green-blue infrastructure is one of the two key areas essential for enhancing environmental resilience. Blue-green infrastructure includes the ‘green’ and ‘blue’ features of our towns and cities that provide environmental benefits and contribute to our quality of life.\nIn recent years, London has increased its focus on urban resilience and green infrastructure, as reflected in various policies. The London Plan 2021, in Chapter 8 (Green Infrastructure), outlines specific requirements for green infrastructure development:\n-Policy G1 Green infrastructure\nLondon’s network of green and open spaces, and green features in the built environment, should be protected and enhanced. Development Plans and area-based strategies should identify key green infrastructure assets, along with their existing and potential functions.\n-Policy G7 Trees and woodlands\nLondon’s urban forest and woodlands should be protected and maintained, and new trees and woodlands should be planted in appropriate locations in order to increase the extent of London’s urban forest\nAlthough the London City Resilience Strategy 2020 does not set explicit targets for green-blue infrastructure, it emphasizes the importance of data in infrastructure planning and advocates for improving the resilience of London’s infrastructure systems while prioritizing investment through the use of data.\n\n\n4.2 Application\n.Remote sensing imagery plays a crucial role in supporting urban green infrastructure planning and policy responses. For example, it can be used to quantify urban forest structure, providing governments with accurate data support. The study 《Quantifying urban forest structure with open-access remote sensing data sets》uses a random forest approach to train the model and apply it to urban forest structure assessment. It first calculates canopy cover, canopy height, and tree density using LiDAR data from the UK Environment Agency. Then, it incorporates Sentinel-2 satellite imagery, climate, and terrain data, which are preprocessed via Google Earth Engine. The random forest model predicts forest structure indicators and is applied at both 100m and 20m grid scales. Additionally, the study validates the model’s accuracy using iTree Eco data and explores its transferability across different urban environments.\n\nMoreover, remote sensing can identify potential areas for urban tree planting, assess the economic benefits of green infrastructure development, and provide data-driven support for optimizing urban green space layouts. The study《Efficient assessments of urban tree planting potential within or near the southern Piedmont region of the United States》employs supervised classification to analyze remote sensing imagery, categorizing land into four classes: water, developed land, open land, and forest. Using NAIP imagery as reference data, the study validates classification accuracy through random sampling. When evaluating suitable planting areas, it excludes obstacles such as buildings, roads, and water bodies, as well as small forest gaps. The proportion of plantable land within open spaces is calculated for each city. For other cities, the study applies the same classification method to identify open areas and estimates total tree planting potential by extrapolating from similar urban areas.\nThese studies demonstrate that remote sensing technology can provide efficient and accurate data support, enabling governments to develop more informed urban green space plans, optimize green infrastructure layouts, and assess their environmental and economic benefits\n\n\n4.3 Reflection\nRemote sensing imagery can be used for monitoring urban forest structures and identifying potential planting areas, providing support for the protection and development of urban green infrastructure. Due to its spatial characteristics, remote sensing imagery offers a new perspective for urban management.\nThrough this week’s learning, I believe that remote sensing imagery can also be utilized to evaluate urban green infrastructure development. Currently, many regions in China are implementing programs such as the Conversion of Farmland to Forest Program and the Permanent Basic Farmland Protection Program. The central and local governments set quantitative targets for cities regarding green infrastructure development and conservation. However, some cities, due to economic development needs, are unable to meet these requirements on their own. To fulfill the mandated targets, they collaborate with other cities through a mechanism known as “horizontal ecological compensation.”For example, if a provincial government requires City A to expand its forest area by 10 square kilometers within a year, but City A only has 5 square kilometers of available land for afforestation, it can compensate by expanding 5 square kilometers of forest in City B. In return, City A provides City B with financial compensation or other benefits to meet the provincial government’s requirements. I believe that remote sensing imagery can first be used to monitor whether local governments have met the green infrastructure targets set by higher authorities. Additionally, it can be employed to evaluate whether this “horizontal ecological compensation” mechanism effectively achieves ecological restoration.\n\n\n6.4 References\nBaines, O. et al. (2020) Quantifying urban forest structure with open-access remote sensing data sets. Urban forestry & urban greening. [Online] 50126653-.\nMerry, K. et al. (2013) Efficient assessments of urban tree planting potential within or near the southern Piedmont region of the United States. Computers, environment and urban systems. [Online] 3939–47."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "casa0023_learning_diary",
    "section": "",
    "text": "This is a Quarto website.\nTo learn more about Quarto websites visit https://quarto.org/docs/websites."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "week1.html",
    "href": "week1.html",
    "title": "week7",
    "section": "",
    "text": "1.1 Summary\nThis week’s lecture covered several key topics: how to create a spectral signature in QGIS, generate a raster stack, and create a true color composite. Additionally, I learned how to adjust image color display and create a true color composite in SNAP software. In R, I explored extracting, analyzing, and visualizing spectral reflectance from different land cover types using Sentinel-2 and Landsat 8-9 satellite imagery to compare spectral signatures.\nSome key points include: Sentinel-2 contains 13 bands, while Landsat 8-9 contains 11 bands. Since the overall resolution of the two datasets differs, and even within the same sensor, different bands have different resolutions, resampling is required before processing. In practical applications, true color images help intuitively understand surface features, especially for non-experts. Creating spectral signatures allows for more precise identification of land cover types, making them useful for land use classification. When identifying specific land cover types, especially small areas, adjusting image color display enhances differentiation and recognition. Comparing spectral reflectance characteristics across land cover types provides a reference for subsequent land use classification.\n\n\n1.2 Application\nThe calculation of remote sensing indices can be applied to land use studies, such as identifying urban built-up areas. Due to the long time span of remote sensing imagery, it is also suitable for longitudinal studies, such as analyzing urban expansion over time. Additionally, because remote sensing images provide global coverage, they can be used for large-scale and regional comparative studies. Overall, Landsat has a longer time span than Sentinel-2, making it suitable for longitudinal studies, while Sentinel-2 has more spectral bands, making it more appropriate for high-precision studies, such as identifying vegetation differences.\nFor identifying the distribution and expansion of urban land (impervious surfaces), Landsat datasets offer long time series, wide spatial coverage, high standardization, and consistency, as well as relatively high spatial resolution, making them suitable for this type of research.\nXiaoping Liu et al. proposed the Normalized Urban Area Composite Index (NUACI), based on multi-temporal global urban land maps derived from Landsat imagery every five years from 1990 to 2010, to create 30m resolution global built-up area maps. The study classified cities based on population size and economic conditions and introduced NUACI by combining the Normalized Difference Water Index (NDWI), Normalized Difference Vegetation Index (NDVI), and Normalized Difference Built-up Index (NDBI). NUACI’s central points and segmentation thresholds were calibrated according to different urban ecological zones.\nAdditionally, I gained the following insights from this paper:\n\nCalibrating key indices by dividing urban ecological zones to improve accuracy\n\nThis study considers both natural and socio-economic differences by incorporating biome types, urban topology, and economic development levels when defining urban ecological zones.\n\n\nHowever, I still have some questions regarding this paper:\n\nDoes NUACI exhibit significant accuracy differences when applied to different remote sensing datasets? If so, how should adjustments be made?\nIs the use of a binary mask from DMSP-OLS nighttime light data in NUACI calculations too rigid? Could it overlook unlit artificial infrastructure?\n\nFor tropical regions with high rainfall, frequent cloud cover, and complex vegetation types, Sentinel-2 datasets, with high spatial resolution, multispectral capabilities, and frequent revisit times, are well-suited for research needs. This study explores whether combining remote sensing data from both the dry and wet seasons enhances land use classification accuracy. The study classifies remote sensing datasets into four categories:\n\nPure dry-season dataset\nPure wet-season dataset\nFull-year dataset\nCombined dry-season and wet-season dataset\n\nEach dataset was trained using four different classifiers: MLR, IK-NN, SVM, and RF, and the results were compared. The study focuses on tropical regions with complex climate conditions (frequent cloud cover and fog), rapidly changing vegetation types, and intricate land use structures.\nAdditionally, I gained the following insights from this paper:\n\nImage Preprocessing\n\nThe Earth’s orbital motion affects solar illumination geometry, which influences vegetation reflectance.\nThis study applied several preprocessing steps before analysis: cloud and shadow removal, BRDF and terrain correction, median compositing, resampling, and image cropping.\n\nSample Data Selection\n\nCompared to other studies, this research used a more comprehensive and objective sample dataset, including field observations, target sampling, and simple random sampling using ultra-high-resolution imagery.\n\nAccuracy Assessment Metrics\n\nThe study used overall accuracy (OA), Kappa coefficient (K), producer’s accuracy (PA), and user’s accuracy (UA) to evaluate classification accuracy.\nFactors affecting classification accuracy include sensor type, sources of training and accuracy assessment data, the number of classes, and the classification method.\n\n\n\n\n1.3 Reflection\nThis week, I have gained a renewed understanding that remote sensing can be achieved through various types of sensors, not just satellites. As far as I know, high-resolution remote sensing imagery is often very expensive, and for small-area studies, purchasing an entire tile may not be necessary, as they typically cover hundreds of kilometers. Perhaps drones can enhance the ability of local agencies to acquire small-area data while also improving data update frequency. Developing general analytical models for drone-acquired imagery might contribute to data empowerment in local governance.\nHowever, I still have a question. Currently, the time span of a single sensor rarely exceeds twenty years. For long-term studies, such as analyzing changes in China’s forest area since 1980, it is necessary to compare imagery collected by different sensors. The resampling methods I learned this week can indeed address differences in the number of bands and resolution between sensors. However, how can we overcome differences in the accuracy and sensitivity of sensors in detecting reflected waves? How do such precision differences impact research results, and how can we minimize their effects? These are the questions I hope to explore further.\n\n\n1.4 References\nLiu, X. et al. (2018) High-resolution multi-temporal mapping of global urban land using Landsat images based on the Google Earth Engine Platform. Remote sensing of environment. [Online] 209227–239.\nNguyen, H. T. T. et al. (2020) Land Use/Land Cover Mapping Using Multitemporal Sentinel-2 Imagery and Four Classification Methods—A Case Study from Dak Nong, Vietnam. Remote sensing (Basel, Switzerland). [Online] 12 (9), 1367-."
  },
  {
    "objectID": "week3.html",
    "href": "week3.html",
    "title": "week3",
    "section": "",
    "text": "3.1 Summary\nThis week’s lecture focuses on how landsat operates and images correction\n1. Digital Multispectral Camera (MSS) \ni．The Standard for Remote Sensing \ni) MSS has become the standard for remote sensing. \nii) MSS performs better than RBV. \nMSS uses a whisk broom scanning method, which ensures uniform data acquisition, making it easier to mosaic and analyze. A single scan covers a large surface area and includes near-infrared bands. \nRBV, on the other hand, functions like an instantaneous photographic camera, capturing a smaller area per shot. It is more susceptible to cloud cover, changes in viewing angles, and unstable data acquisition, primarily operating in the visible spectrum. \nii．Whisk Broom vs. Push Broom \n- Whisk Broom: Uses a mechanically oscillating mirror to scan the ground laterally, acquiring one pixel at a time. It requires line-by-line scanning and relies on a single detector. Scan Line Correction is crucial. \n- Push Broom: Utilizes a linear CCD detector array, eliminating the need for a mechanical scanning mirror. It captures an entire scan line at once with multiple detectors. \n 2. Geometric Correction \ni．Causes of Image Distortions \ni) View angle (off-nadir) \nii) Topography \niii) Wind (for aerial imagery) \niiii) Earth’s rotation \nii．How to Correct Geometric Distortions \ni) Identify Ground Control Points (GCPs) using GPS. \nii) Apply linear regression to compute geometric transformation coefficients. \niii) Minimize the Root Mean Square Error (RMSE). \niv) Compare different transformation algorithms. \niii．Method 1: Forward Mapping \n- input to output (forward mapping), for the new x\n- X=the rectified x and y which could fall anywhere on the gold standard map——Interpolation\nIv．Method 2: Backward Mapping \n- output to input (backward mapping), for the new x:\n- Xi（Original image ）=Can correct the distortion of the original image\nV．Resampling Methods \ni) Nearest Neighbor \nii) Bilinear Interpolation \niii) Cubic Convolution \niv) Cubic Spline \n3. Atmospheric Correction \ni．Causes of Atmospheric Distortion \ni) Atmospheric Scattering \n   - Path Radiance \n   - Atmospheric Attenuation \nii) Topographic Attenuation \nii. Relative Correction Methods \n- Dark Object Subtraction (DOS) – Used for visible and short-wave infrared correction. \n- FLAASH Analytical Correction – Suitable for multispectral imagery. \n- Pseudo-Invariant Features (PIFs) – Used for multi-temporal image correction. \niii.  Absolute Correction Methods \n- Converts digital brightness values into scaled surface reflectance. \ni) Atmospheric Radiative Transfer Models \n   - Requires data such as: \n     - Atmospheric Model \n     - Local Atmospheric Visibility \n     - Image Altitude \nii) Empirical Line Correction (ELC) \niii) Factors Affecting Atmospheric Correction \n4. Orthorectification Correction (A Subset of Georectification) \nEnsures that pixels are viewed at nadir (directly below the sensor). \ni.  Requirements \n- Sensor Geometry \n- An Elevation Model \nii.  Correction Methods \ni) Cosine Correction (Primary method) \nii) Minnaert Correction \niii) Statistical Empirical Correction \niv) C Correction \n\n\n3.2 Application\nCorrection aims to reduce errors. In different scenarios, slight variations can lead to changes in the reflectance of surface features. Correction is applied to address errors caused by these variations. For example, in the infrared band, the reflectance of clear water is close to zero. However, most water bodies contain suspended particles, chlorophyll, and colored dissolved organic matter (CDOM), which affect their reflectance.\nMarcela Pereira-Sandoval focused on the unique characteristics of inland lakes compared to other water bodies and compared the performance of different atmospheric correction processors in various types of inland waters. First, multiple sample water bodies were selected, and based on in situ measurements of chlorophyll-a concentration and Secchi disk depth, the water bodies were classified into three categories: ultra-oligotrophic to oligotrophic, mesotrophic to eutrophic, and hypertrophic. Six atmospheric correction processors with different principles—ACOLITE, C2RCC, C2RCCCX, iCOR, Polymer, and Sen2Cor—were used to process the images. By comparing the corrected image reflectance with in situ measured reflectance, the performance of the processors was evaluated. The study showed that different processors exhibited varying correction effects, and even the same processor performed differently across different types of water bodies. Therefore, accurately understanding surface characteristics is crucial for improving correction quality.\nL. Yan conducted research on geometric correction and proposed a new method for characterizing and improving the geometric distortion of Landsat MSS images using Landsat 8 and 9 OLI data. This provides a new approach for long-term studies combining data from different sensors. The study employed least-squares dense matching (LSM) + a many-to-many matching strategy to establish a network of tie points between MSS images and OLI L1TP images. A second-order polynomial + radial basis function (RBF) was used to derive the mapping relationship, and the images were reprojected using the inverse gridding method + bilinear resampling. Finally, LSM was applied again to verify whether the corrected MSS images were aligned with the OLI images, and RMSE was calculated to assess the correction accuracy.\nCompared to one-to-one and one-to-many matching strategies, the many-to-many matching strategy used in this study provided more matching opportunities, allowing for a more effective capture of geometric distortions between images. However, it is computationally expensive and not suitable for large-scale studies. Although this geometric correction method has been validated in multiple study areas, its global application would require consideration of factors such as terrain variations and cloud cover.\nAfter reading these papers, I still have the following question:\n\nDo atmospheric component concentrations vary with geographic location? If the concentrations differ, will their impact on surface reflectance also vary? If the variation remains approximately the same within a certain range, where is the threshold?\n\n\n\n3.3 Reflection\nWhen performing corrections, we need to understand the causes of errors and adjust the correction methods accordingly. Additionally, we should critically evaluate existing correction methods and select or modify them based on the characteristics of the study area. For example, atmospheric correction methods that assume water bodies have zero reflectance in the infrared band may not be applicable to eutrophic waters.\nI also found that improving correction accuracy and controlling computational costs are often conflicting objectives. In practical applications, a balance must be struck based on research needs. Regions with extreme weather conditions, such as tropical and polar areas, often face greater challenges in atmospheric correction. However, it is important to acknowledge that these areas are often located in developing countries, where there is a lack of technical and financial resources for data processing. In some cases, achieving extremely high accuracy may not be meaningful. Instead, developing a cost-effective correction method that maintains errors within an acceptable range could be more valuable.\n\n\n3.4 References\nYan, L. & Roy, D. P. (2025) Using Landsat 8 and 9 operational land imager (OLI) data to characterize geometric distortion and improve geometric correction of Landsat Multispectral Scanner (MSS) imagery. Remote sensing of environment. [Online] 321114679-.\nPereira-Sandoval, M. et al. (2019) Evaluation of Atmospheric Correction Algorithms over Spanish Inland Waters for Sentinel-2 Multi Spectral Imagery Data. Remote sensing (Basel, Switzerland). [Online] 11 (12), 1469-."
  },
  {
    "objectID": "week6.html",
    "href": "week6.html",
    "title": "week6",
    "section": "",
    "text": "6.1 Summary\nThis week’s lecture focused on Google earth engine\n\nImportant terms of GEE\n\n\nImage = Raster\nFeature = Vector\nImage Collection = a set of time series raster data\nFeature Collection= Multiple vector objects\n\n\nGEEoperation mode\nIt adopts the Client-Server architecture Parallelism is supported, with the for loop executing locally and the map() function processing in parallel Dynamic resolution adjustment, adjusting the resolution according to the zoom level\nPrecautions for data loading\n\n\n\nThe scale parameter determines the resolution of data export When the scale is lower than the original resolution of the image, the true resolution of the image is not increased, but the value of the original pixel. When the scale is higher than the original resolution (such as scale=50), multiple pixels are aggregated and the calculated value may change.\nGEE image loading and visualization need to specify the band and visualization parameters\nGEE will automatically convert the projection when processing remote sensing images, and most data will use EPSG:3857 (Web Mercator) by default.\nGEE main function\n\n\nee.Image: single raster image\nee.ImageCollection: Multiple raster images, such as data sets\nee.Geometry: points, polygons and other basic geometric shapes\nee.Feature: Geometric objects with attribute information, such as a country border data\nee.FeatureCollection: A collection of features\nee.Reducer: Aggregate data\nee.Join: Data merge, such as spatial join.\nee.Array: multidimensional data structure (similar to NumPy)\nee.Chart: An object that visualizes the analysis results\n\n\ntypical processes in GEE\n\n\n\nJoins\nZonal Statistics\nFiltering of images or specific values\nImage\nReduction\n\n\nReduce by Pixel\nReduce by Region Calculate statistics for a specific region\nReduce by Neighborhood\nThe pixel and the pixel around it are calculated to generate a new value\n\nset a window of pixels surrounding a central pixel (3*3, 5*5)\ncalculate like a filter or texture measure\npurpose: Edge Detection, Smoothing, Denoising and so on\n\n\n\n\n\n\n\n\n\n6.2 Application\nThere are many tools available for remote sensing image analysis, each differing in functionality, data processing capability, and applicable domains. To understand the differences between various platforms, I read two papers this week that used different analytical platforms.\nIsabela Xavier Floreano used GEE to study ten years of rainforest LULC changes in a state, performed area statistics using QGIS, and used TerrSet for prediction. Since the study area was large and the time span was long, the training samples for supervised classification were substantial. GEE, as a cloud computing platform, can efficiently process large-scale remote sensing datasets and supports parallel computing, making it well-suited for the computational demands of this study. However, Isabela Xavier Floreano did not perform all operations using GEE alone.\nMarwa Waseem A. Halmy, on the other hand, used only IDRISI Selva to classify and predict land use in a desert area. Unlike GEE, IDRISI Selva is a desktop software that only supports single-machine operation but can function offline. It has built-in analytical tools and does not require programming. Marwa Waseem A. Halmy’s study focused on a relatively small land area, meaning that despite IDRISI Selva’s reliance on local hardware, it was still sufficient for the computational requirements. Overall, GEE has distinct advantages in large-area and long-term studies but requires some programming knowledge. For certain analyses, it may still have limitations compared to specific platforms. Other analytical platforms offer unique advantages, such as built-in tools that do not require programming knowledge.\nIn addition to gaining insights into remote sensing image analysis tools, the papers I read this week also deepened my understanding of land use and land cover (LULC) change prediction methods. Both studies used Markov-CA for LULC prediction. The core concept of the Markov Chain is that if the system’s previous state is known, the probability of the system being in a particular state at a specific time can be determined. The Markov Chain can calculate the frequency of each category’s changes between two periods and the proportion of each category converting to others. Cellular Automata is a grid-based dynamic modeling method that assumes cell changes are influenced by their surrounding areas and is used to simulate spatial change processes.\nBoth studies used driver variables or suitability factors—such as distance to roads, city centers, and water bodies—to determine the likelihood of LULC changes. Isabela Xavier Floreano applied a Multi-Layer Perceptron (MLP) to compute the probability of each pixel being suitable for conversion to a specific LULC category, generating a suitability map. In contrast, Marwa Waseem A. Halmy used WLC (Weighted Linear Combination) and Boolean constraints. Regarding neighborhood influence modeling, MLP inherently learns neighborhood effects and does not explicitly adjust suitability based on neighboring pixels. Meanwhile, Marwa Waseem A. Halmy’s study applied a 5×5 contiguity filter, using weighted averaging to adjust suitability, resulting in smoother prediction outcomes.\nAdditionally, Marwa Waseem A. Halmy’s study employed valuable methods for LULC classification. When using Random Forest for LULC classification, the study incorporated auxiliary data such as slope, terrain roughness index, and topographic wetness index in addition to spectral data, enhancing classification accuracy.\n\n\n6.3 Reflection\nDuring my undergraduate studies, I frequently used ENVI for remote sensing analysis, which required downloading imagery before processing. When working with multi-temporal or large-area datasets, I often encountered performance limitations due to hardware constraints. Additionally, modifying the study area meant re-importing and reprocessing the data, which was time-consuming. In contrast, Google Earth Engine (GEE) provides pre-loaded datasets and runs analyses on Google’s servers with parallel computing, significantly improving processing speed and reducing hardware requirements. Since analyses are conducted through code, they can be easily modified, replicated, and shared on open-source platforms.\nBeyond technical efficiency, GEE also promotes data accessibility and fairness. Remote sensing analysis traditionally requires expensive hardware, making it inaccessible to economically disadvantaged regions. Although satellite imagery covers the entire globe, many underdeveloped areas lack the resources to utilize it. By lowering the technical and financial barriers, GEE allows anyone with basic equipment to perform remote sensing analysis using open-source code.\nAdditionally, GEE Apps enable interactive visualization, making it easier to present complex spatial data to non-experts, such as the general public. Governments can leverage this capability in official reports to enhance transparency and ensure public access to critical information.\n\n\n6.4 References\nHalmy, M. W. A. et al. (2015) Land use/land cover change detection and prediction in the north-western coastal desert of Egypt using Markov-CA. Applied geography (Sevenoaks). [Online] 63101–112.\nFloreano, I. X. & de Moraes, L. A. F. (2021) Land use/land cover (LULC) analysis (2009–2019) with Google Earth Engine and 2030 prediction using Markov-CA in the Rondônia State, Brazil. Environmental monitoring and assessment. [Online] 193 (4), 239-."
  },
  {
    "objectID": "week8.html",
    "href": "week8.html",
    "title": "week7",
    "section": "",
    "text": "8.1 Summary\nThis week’s lecture introduced a new land cover classification dataset called Dynamic World based on a Semi-supervised approach, Object-Based Image Analysis, Sub-pixel analysis – two new classification methods – and methods for classification accuracy assessment.\n1.  A New Real-time Land Cover Classification Dataset – Dynamic World\n\nData Classification Method\n\n\nThe dataset adopts a semi-supervised classification approach, which combines both supervised and unsupervised learning.\nThe globe is divided into 14 biomes, and sampling is conducted according to geographic regions and biomes.\nAnnotators classify Surface Reflectance and assign different weights based on professional expertise.\nTop of Atmosphere Reflectance is used for training.\n\n\nAdvantages\n\n   It features continuously evolving predictions based on real-time data input and is constantly improved through deep learning methods, thereby enhancing the accuracy and relevance of the predictions.\n\nDisadvantages\n\n\nThe data is not clear at local scales and is only suitable for large-scale studies (national, global scale).\nExcessive preprocessing and filtering lead to data smoothing; the combination of the 50×50m minimum mapping unit and the moving window of the CNN may cause blurring at local scales due to information integration and spatial compression.\n\n2. Object-Based Image Analysis (OBIA)\n\nDefinition\n\n   It aggregates similar pixels into superpixels, and then these objects are used for classification or analysis.\n\nMethod – Simple Linear Iterative Clustering\n\n\nIt calculates spatial distance + color similarity, iteratively optimizing the segmentation results and adjusting the superpixel centers.\nParameters affecting the shape and distribution of superpixels: The S value (the distance between initial points) determines the size of the superpixels, and the m value (compactness) controls the balance between spatial distance and color similarity.\n\n3.  Sub-pixel Analysis\n\nDefinition\n\n   It assumes that a single pixel is composed of multiple types of land cover.\n\nMethods\n\n\nSelect Spectrally Pure Endmembers (e.g., water, vegetation, soil).\nUse Linear Spectral Unmixing (LSU) to calculate the proportion of each endmember.\n\n\nShortage\n\n   There are errors because it is very difficult to find completely pure endmembers.\n4. Accuracy Assessment\n\nConfusion Matrix\n\n   Producer’s Accuracy = TP / (TP + FN)\n   User’s Accuracy =TP/ (TP + FP)\n   Overall Accuracy = (TP +TN)/(TP+FP+FN+TN)\nWe should choose the appropriate data and methods based on the research objectives and scale. Pixel-based classification is suitable for large-scale, low-resolution data. Object-based classification is suitable for high-resolution data. Sub-pixel analysis is suitable for mixed land cover areas, such as urban regions.\n\n\n8.2 Application\nDifferent classification methods result in variations in classification accuracy and the spatial patterns of classification results. Depending on the research objective, different classification methods can be combined with various spatial analysis techniques. The sub-pixel model assumes that a single pixel contains multiple land cover types, which can compensate for the limitations of image resolution and provide high-accuracy estimates based on low-resolution data. For example, Narumasa Tsutsumida monitored impervious surface areas (ISA) in Jakarta using MODIS data.Object-based classification produces contiguous and block-like classification results, which can be converted into points to calculate density for spatial statistical analysis. For example, the study by Ngoc Tuyen Nguyen et al. combined object-based classification with kernel density estimation (KDE) to analyze the spatial distribution of impervious surfaces in Ho Chi Minh City, identifying hotspot regions and temporal changes.\nNarumasa Tsutsumida’s research focused on developing a classification model to overcome the challenges of mixed pixels and low resolution. The study consisted of four main stages: data preprocessing, reference data collection, model development, and model comparison. First, the data underwent preprocessing, where cloud-covered areas were masked out, and missing values were interpolated using the double logistic function in TIMESAT to smooth the EVI time series. A total of 1,000 regions were selected, and the ISA proportion within each region was estimated based on high-resolution imagery. The reference data was then divided into training and validation sets. A random forest regression model was used, with EVI values as input variables and ISA proportions as output variables. Finally, a pixel classification model was constructed based on different thresholds (25%, 50%, and 75%), and the accuracy differences were compared using overall accuracy and %RMSE as evaluation metrics.\nNgoc Tuyen Nguyen’s study on impervious surfaces in Ho Chi Minh City was divided into two main parts: land use classification and spatial statistical analysis. The land use classification process included three steps: spectral index calculation, segmentation, and classification. First, the modified normalized difference water index (MNDWI), enhanced normalized difference impervious surface index (ENDISI), and normalized difference vegetation index (NDVI) were calculated based on preprocessed imagery. The images were then segmented into small single-pixel objects, which were merged based on similarity or local homogeneity. The segmentation process was controlled by compactness, scale parameter, and shape parameters. Using the class hierarchy method, land cover was classified into impervious areas, bare land, and other based on historical spectral index thresholds, location, shape, texture, and other statistical information.After classification, impervious surface pixels were converted into point data, and the KDE method was applied to calculate impervious surface density. Spatial statistical analyses were then performed based on the density data, including spatial autocorrelation analysis, optimized hotspot analysis, and emerging hotspot analysis. Spatial autocorrelation analysis was used to detect the spatial dependence of observations, optimized hotspot analysis identified clusters of ISA hotspots and cold spots, and emerging hotspot analysis tracked temporal changes in ISA hotspots.\n\n\n8.3 Reflection\nIn my undergraduate thesis, I studied the spatial distribution characteristics of land expansion in Shenzhen. At that time, I used a relatively crude method, which divided the city into eight sectors based on the urban geometric center and calculated the differences in newly added land among these sectors. This method had limitations in accuracy, as it could not precisely identify the specific locations of newly added land. Instead, it only reflected the overall scale of land expansion and failed to capture changes in the number of new development projects. For example, some areas might have a large number of small-scale development projects, while others might have only a few large-scale projects, and this method could not reveal such differences.\nDuring this week’s study, I learned about object-based classification methods and came across research that combines object-based classification with spatial statistical analysis. This approach converts classified objects into point data for further statistical analysis. I believe this method can better optimize my previous research and allow for a more in-depth exploration of boundary differences in land development.\n\n\n8.4 References\nNguyen, N. T. et al. (2024) Identifying impervious surface hot spots in Ho Chi Minh City using object-based classification and spatial statistics. IOP conference series. Earth and environmental science. [Online] 1349 (1), 12030-.\nTsutsumida, N. et al. (2016) Sub-Pixel Classification of MODIS EVI for Annual Mappings of Impervious Surface Areas. Remote sensing (Basel, Switzerland). [Online] 8 (2), 143-."
  }
]